{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T01:35:45.119784Z",
     "start_time": "2019-08-06T01:35:45.106363Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import random\n",
    "from tensorflow.contrib.layers import fully_connected \n",
    "import math\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='-1'\n",
    "timewindow=72"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#sepsis auclos 59.20153714773698  non_sepsis 37.37243960303405"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1、导入数据并且清理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T01:35:48.487794Z",
     "start_time": "2019-08-06T01:35:48.447650Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    setAB_data=np.load(filename)\n",
    "    setAB_X_t=setAB_data['X_t']   \n",
    "    setAB_X_last_obsv=setAB_data['X_last_obsv'] \n",
    "\n",
    "    setAB_X_t_mask=setAB_data['X_t_mask']\n",
    "    setAB_deltaT_t=setAB_data['deltaT_t']\n",
    "    setAB_label=setAB_data['label']\n",
    "\n",
    "    return setAB_X_t,setAB_X_last_obsv,setAB_X_t_mask,setAB_deltaT_t, setAB_label\n",
    "\n",
    "\n",
    "def pad_matrix(x):\n",
    "    n = len(x)\n",
    "    t = timewindow\n",
    "    d = x[0].shape[1]\n",
    "    \n",
    "    ret = np.zeros([n, t, d], dtype=float)   \n",
    "    length=[len(i) for i in x]\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        if len(x[i])<t:\n",
    "            ret[i, :length[i],:] = x[i]\n",
    "        else:\n",
    "            ret[i,:,:] = x[i][length[i]-t:length[i],:]         \n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_Normalization(X):\n",
    "\n",
    "    X_mean=np.array([84.54736749058092, 97.19735567887244, 36.97384942767003, 123.79455870514964, 82.39844634983528, \n",
    "                     63.82168093649573, 18.720210390214252, 33.01391751459007, -0.6869669057468529, 24.07165352824123,\n",
    "                     0.5629699098557109, 7.378730326999917, 41.08205268057132, 92.6638691105628, 269.71012201748755, \n",
    "                     23.989203500427596, 103.83241100807658, 7.578440945745791, 105.83631589184984, 1.5132853634707293, \n",
    "                     1.7982557651991606, 137.10754340757103, 2.666527938225818, 2.0523013086498567, 3.5532881933995384, \n",
    "                     4.138466653390937, 2.1437651299155416, 8.152533277169335, 30.79291672353087, 10.43255372477224, \n",
    "                     41.26944061491329, 11.458261650095405, 284.57097280966764, 196.1678411769487, 62.02033203772457, \n",
    "                     0.5577890176790509, 0.49908857795492095, 0.5009114220450791, -55.728074496237774, 26.93293913918774]) \n",
    "    \n",
    "    X_std=np.array([17.357143533110786, 2.9197410787621823, 0.769008502635353, 23.226370766088404, 16.35175339480381, \n",
    "                   13.990094006590905, 5.096634488705786, 7.901376008166723, 4.288142385817428, 4.3942700623947095, \n",
    "                   12.460897950458373, 0.07483160020802701, 9.327999041120131, 10.841774762519147, 881.945227133347,\n",
    "                   20.067454723313723, 125.78376567914378, 2.420980582249712, 5.856066574937517, 1.8016280879794837,\n",
    "                   3.555075159474239, 51.31864530748081, 2.5840546941276425, 0.4008345314433298, 1.438200307219221, \n",
    "                   0.6449469067770479, 4.358151926253383, 24.455704359390893, 5.491328159692898, 1.9666320193417495, \n",
    "                   26.26311427074314, 7.737594668485472, 150.0117749185703, 103.8931301983612, 16.41032814994427, \n",
    "                   0.49664920158567677, 0.4999991693091656, 0.4999991693091656, 161.2959477283614, 28.699759200381497])\n",
    "    \n",
    "    num_patient=X.shape[0]    \n",
    "    for i in range(num_patient):    \n",
    "        X[i] = (X[i] - X_mean)/X_std\n",
    "    return X\n",
    "\n",
    "def get_minibatch(index,mb_size,X_t,X_last_obsv,X_t_mask,deltaT_t,label):   \n",
    "    \n",
    "    X_t = X_t[index:index+mb_size]\n",
    "    X_last_obsv=X_last_obsv[index:index+mb_size]\n",
    "    X_t_mask = X_t_mask[index:index+mb_size]\n",
    "    deltaT_t = deltaT_t[index:index+mb_size]\n",
    "    label = label[index:index+mb_size]\n",
    "    \n",
    "    seq_len=np.array([len(X_t[i]) for i in range(len(X_t))])\n",
    "    \n",
    "    seq_len=np.where(seq_len>timewindow,timewindow,seq_len)\n",
    "    \n",
    "    X_t=pad_matrix(get_Normalization(X_t))\n",
    "    X_last_obsv=pad_matrix(get_Normalization(X_last_obsv))\n",
    "\n",
    "    X_t_mask=pad_matrix(X_t_mask)\n",
    "    deltaT_t=pad_matrix(deltaT_t)\n",
    "    label=pad_matrix(label)\n",
    "\n",
    "    label_mask=np.zeros_like(label, dtype=int)\n",
    "    \n",
    "    for i in range(label.shape[0]):\n",
    "        label_mask[i,:seq_len[i],:]=1\n",
    "        \n",
    "    return  (X_t, X_last_obsv,X_t_mask , deltaT_t,label,seq_len,label_mask)\n",
    "\n",
    "def get_val_data(filename):\n",
    "    \n",
    "    setAB_data=np.load(filename)\n",
    "    setAB_X_t=setAB_data['X_t']   \n",
    "    setAB_X_last_obsv=setAB_data['X_last_obsv'] \n",
    "\n",
    "    setAB_X_t_mask=setAB_data['X_t_mask']\n",
    "    setAB_deltaT_t=setAB_data['deltaT_t']\n",
    "    setAB_label=setAB_data['label']\n",
    "    \n",
    "    seq_len=np.array([len(setAB_X_t[i]) for i in range(len(setAB_X_t))])\n",
    "    \n",
    "    seq_len=np.where(seq_len>timewindow,timewindow,seq_len)\n",
    "    \n",
    "    X_t=pad_matrix(get_Normalization(setAB_X_t))\n",
    "    X_last_obsv=pad_matrix(get_Normalization(setAB_X_last_obsv))\n",
    "\n",
    "    X_t_mask=pad_matrix(setAB_X_t_mask)\n",
    "    deltaT_t=pad_matrix(setAB_deltaT_t)\n",
    "    label=pad_matrix(setAB_label)\n",
    "    \n",
    "    label_mask=np.zeros_like(label, dtype=int)\n",
    "    \n",
    "    for i in range(label.shape[0]):\n",
    "        label_mask[i,:seq_len[i],:]=1\n",
    "    \n",
    "    return (X_t,X_last_obsv, X_t_mask, deltaT_t,label,seq_len,label_mask)\n",
    "    \n",
    "def suffle_data(X_t,X_last_obsv,X_t_mask,deltaT_t,label):\n",
    "    batch=X_t.shape[0]\n",
    "    random_index=np.arange(batch).tolist()\n",
    "    shuffle=random.sample(random_index,batch) \n",
    "    X_t=X_t[shuffle]\n",
    "    X_last_obsv=X_last_obsv[shuffle]\n",
    "    X_t_mask=X_t_mask[shuffle]\n",
    "    deltaT_t=deltaT_t[shuffle]\n",
    "    label=label[shuffle]\n",
    "    return X_t,X_last_obsv,X_t_mask,deltaT_t,label   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2、设计网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T02:54:22.780340Z",
     "start_time": "2019-08-06T02:54:22.687207Z"
    }
   },
   "outputs": [],
   "source": [
    "def FilterLinear(input1,in_features, out_features, filter_square_matrix):\n",
    "    stdv = 1./ math.sqrt(in_features)\n",
    "    weight=tf.Variable(tf.random.uniform((out_features,in_features),minval=-stdv, maxval=stdv,dtype=tf.float32))  \n",
    "    bias=tf.Variable(tf.random.uniform((out_features,),minval=-stdv, maxval=stdv,dtype=tf.float32))\n",
    "    \n",
    "    temp=filter_square_matrix*weight\n",
    "    result=tf.add(tf.matmul(input1,temp),bias)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def FCNet(inputs,h_dim,o_fn):     \n",
    "    layers  = fully_connected (inputs=inputs, num_outputs=h_dim, activation_fn=o_fn)\n",
    "    return layers\n",
    "\n",
    "\n",
    "class Model_GRUD:\n",
    "    def __init__(self, sess,name,input_size):\n",
    "        self.sess=sess\n",
    "        self.name=name  \n",
    "        \n",
    "        self.input_size=input_size\n",
    "        self.delta_size = input_size      \n",
    "        self.mask_size = input_size                   \n",
    "        self.hidden_size=input_size\n",
    "               \n",
    "        self.forward()\n",
    "        \n",
    "\n",
    "    def grud_cell(self, x, x_last_obsv, x_mean, h, mask, delta):\n",
    "        \n",
    "        input_temp= tf.stack((self.zeros_matrix,FilterLinear(delta,self.delta_size, self.delta_size, self.identity)),axis=1)\n",
    "\n",
    "     \n",
    "        delta_x = tf.exp(-tf.reduce_max(input_temp,reduction_indices=1))\n",
    "        \n",
    "        hidden_temp=tf.stack((self.zeros_matrix,FCNet(delta,self.delta_size,None)),axis=1)    \n",
    "        delta_h = tf.exp(-tf.reduce_max(hidden_temp,reduction_indices=1))    \n",
    "        \n",
    "        \n",
    "        x = mask * x + (1 - mask) * (delta_x * x_last_obsv + (1 - delta_x) * x_mean)       \n",
    "        h = delta_h * h\n",
    "\n",
    "        combined = tf.concat((x, h, mask), axis=1)\n",
    "\n",
    "        \n",
    "        weight=tf.Variable(tf.random.normal([self.input_size+self.hidden_size+self.mask_size,self.hidden_size], \n",
    "                                                      stddev=0.01))  \n",
    "        bias=tf.Variable(tf.zeros([self.hidden_size]))\n",
    "        z=tf.nn.sigmoid(tf.add(tf.matmul(combined,weight),bias))\n",
    "        \n",
    "        weight2=tf.Variable(tf.random.normal([self.input_size+self.hidden_size+self.mask_size,self.hidden_size], \n",
    "                                                       stddev=0.01)) \n",
    "        bias2=tf.Variable(tf.zeros([self.hidden_size]))  \n",
    "        \n",
    "        r=tf.nn.sigmoid(tf.add(tf.matmul(combined,weight2),bias2))\n",
    "               \n",
    "        combined_r = tf.concat((x, r * h, mask), axis=1)\n",
    "\n",
    "        weight3=tf.Variable(tf.random.normal([self.input_size+self.hidden_size+self.mask_size,self.hidden_size], \n",
    "                                                       stddev=0.01))   \n",
    "        bias3=tf.Variable(tf.zeros([self.hidden_size]))\n",
    "        \n",
    "        h_tilde=tf.nn.tanh(tf.add(tf.matmul(combined_r,weight3),bias3))\n",
    "                 \n",
    "        h = (1 - z) * h + z * h_tilde\n",
    "\n",
    "        return h\n",
    "    \n",
    "    def forward(self):      \n",
    "\n",
    "        self.X_t         = tf.placeholder(tf.float32, shape=[None,None, 40], name='X_t')\n",
    "        self.X_last_obsv = tf.placeholder(tf.float32, shape=[None,None, 40], name='X_last_obsv')\n",
    "        self.X_t_mask    = tf.placeholder(tf.float32, shape=[None,None, 40], name='mask')     \n",
    "        self.deltaT_t    = tf.placeholder(tf.float32, shape=[None,None, 40], name='timestamp') \n",
    "        \n",
    "        self.labels      = tf.placeholder(tf.float32, shape=[None,None, 1], name='label')\n",
    "        self.label_mask  = tf.placeholder(tf.float32, shape=[None,None, 1], name='label') \n",
    "        \n",
    "        \n",
    "        self.batch_size  = tf.placeholder(tf.int32, [], name='batch_size')\n",
    "        self.seq_length  = tf.placeholder(tf.float32, shape=[None,], name='label')\n",
    "        \n",
    "        self.max_length  = timewindow\n",
    "        \n",
    "        self.identity    = tf.eye((self.input_size)) \n",
    "        self.zeros_matrix= tf.zeros((self.batch_size,self.delta_size))         \n",
    "        self.X_mean      = tf.zeros((self.batch_size,self.input_size))         \n",
    "        Hidden_State     = tf.zeros((self.batch_size, self.hidden_size))\n",
    "\n",
    "        outputs_final = [] \n",
    "        \n",
    "        for i in range(self.max_length):\n",
    "            Hidden_State =self.grud_cell(tf.squeeze(self.X_t[:, i:i + 1, :], axis=1),\n",
    "                                     tf.squeeze(self.X_last_obsv[:, i:i + 1, :], axis=1),\n",
    "                                     tf.squeeze(self.X_mean),\n",
    "                                     Hidden_State,\n",
    "                                     tf.squeeze(self.X_t_mask[:, i:i + 1, :], axis=1),\n",
    "                                     tf.squeeze(self.deltaT_t[:, i:i + 1, :], axis=1))\n",
    "            \n",
    "        \n",
    "            outputs_temp = FCNet(Hidden_State,1,tf.nn.sigmoid)\n",
    "            outputs_final.append(outputs_temp) \n",
    "            \n",
    "        self.outputs= tf.stack(outputs_final, axis=1)            \n",
    "           \n",
    "        ##加上label的mask，对于那些人为补齐的序列不去计算它的损失\n",
    "        self.outputs2=self.label_mask*self.outputs\n",
    "        \n",
    "\n",
    "        ## 加上1e-20是为了防止log(0)bug,导致loss为nan\n",
    "        self.loss = -tf.reduce_sum(tf.reduce_sum(self.labels*tf.math.log(tf.add(self.outputs2,1e-20))+        \n",
    "                                (1-self.labels)*tf.math.log(1-self.outputs2),1))/ (tf.reduce_sum(self.seq_length))\n",
    "             \n",
    "        self.solver = tf.train.AdamOptimizer(learning_rate=0.001).minimize(self.loss)                                             \n",
    " \n",
    "\n",
    "    def train(self, batch_data):                 \n",
    "        _,current_loss=self.sess.run([self.solver, self.loss], \n",
    "                                    feed_dict={self.X_t: batch_data[0],         self.X_last_obsv: batch_data[1], \n",
    "                                               self.X_t_mask:batch_data[2],     self.deltaT_t: batch_data[3],\n",
    "                                               self.labels:batch_data[4],       self.batch_size:batch_data[0].shape[0],\n",
    "                                               self.seq_length:batch_data[5],   self.label_mask:batch_data[6]})\n",
    "        return current_loss\n",
    "\n",
    "    def validation(self, val_data): \n",
    "        current_loss=self.sess.run(self.loss, \n",
    "                                    feed_dict={self.X_t: val_data[0],            self.X_last_obsv: val_data[1], \n",
    "                                               self.X_t_mask:val_data[2],        self.deltaT_t: val_data[3],\n",
    "                                               self.labels:val_data[4],          self.batch_size:val_data[0].shape[0],\n",
    "                                               self.seq_length:val_data[5],      self.label_mask:val_data[6]})\n",
    "        return current_loss\n",
    "        \n",
    "    def predict(self, test_data):  \n",
    "        output=self.sess.run(self.outputs, \n",
    "                             feed_dict={self.X_t: test_data[0],                  self.X_last_obsv: test_data[1], \n",
    "                                        self.X_t_mask:test_data[2],              self.deltaT_t: test_data[3],\n",
    "                                        self.batch_size:test_data[0].shape[0]})\n",
    "        return output   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3、下载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T01:40:55.146536Z",
     "start_time": "2019-08-06T01:39:38.675424Z"
    }
   },
   "outputs": [],
   "source": [
    "X_t,X_last_obsv,X_t_mask,deltaT_t,label = load_data('/data/jiawenxiao/physionet0727/train_data.npz')\n",
    "val_data=get_val_data('/data/jiawenxiao/physionet0727/val_data.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4、 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T03:10:23.493345Z",
     "start_time": "2019-08-06T03:05:18.163721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|| ITR: 0001 | Train Loss: 0.1002  Val Loss 0.0690\n",
      "|| ITR: 0002 | Train Loss: 0.0667  Val Loss 0.0667\n",
      "|| ITR: 0003 | Train Loss: 0.0639  Val Loss 0.0658\n",
      "|| ITR: 0004 | Train Loss: 0.0618  Val Loss 0.0658\n",
      "|| ITR: 0005 | Train Loss: 0.0602  Val Loss 0.0649\n",
      "|| ITR: 0006 | Train Loss: 0.0586  Val Loss 0.0652\n"
     ]
    }
   ],
   "source": [
    "input_size=40\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "sess = tf.Session(config=config)\n",
    "model= Model_GRUD(sess,'GRUD',input_size)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batch_size=256\n",
    "iters=7\n",
    "batch=X_t.shape[0]\n",
    "  \n",
    "for itr in range(1,iters):\n",
    "    X_t_shuffle,X_last_obsv_shuffle,X_t_mask_shuffle,deltaT_t_shuffle,label_shuffle=suffle_data(X_t,\n",
    "                                                                 X_last_obsv,X_t_mask,deltaT_t,label)\n",
    "    total_loss=0  \n",
    "    num=0\n",
    "    for i in range(0,batch-batch_size,batch_size):\n",
    "        batch_data=get_minibatch(i,batch_size,X_t_shuffle,X_last_obsv_shuffle,X_t_mask_shuffle,deltaT_t_shuffle,label_shuffle)\n",
    "        curr_loss= model.train(batch_data)  \n",
    "        total_loss+=curr_loss\n",
    "        num+=1     \n",
    "    val_loss= model.validation(val_data)  \n",
    "    avg_loss=total_loss/num\n",
    "    print('|| ITR: ' + str('%04d' % (itr)) + ' | Train Loss: ' +str('%.4f' %(avg_loss))+'  Val Loss '+str('%.4f' %(val_loss)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5、测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T04:19:56.602245Z",
     "start_time": "2019-08-06T04:19:56.574214Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_filename(path,filetype):\n",
    "    name =[]\n",
    "    final_name = []\n",
    "    for root,dirs,files in os.walk(path):\n",
    "        for i in files:\n",
    "            if filetype in i:\n",
    "                name.append(i.replace(filetype,''))#生成不带‘.csv’后缀的文件名组成的列表\n",
    "    final_name = [item +'.psv' for item in name]#生成‘.csv’后缀的文件名组成的列表\n",
    "    return final_name\n",
    "\n",
    "def save_challenge_predictions(file, scores, labels):\n",
    "    with open(file, 'w') as f:\n",
    "        f.write('PredictedProbability|PredictedLabel\\n')\n",
    "        for (s, l) in zip(scores, labels):\n",
    "            f.write('%g|%d\\n' % (s, l))\n",
    "\n",
    "def pad_matrix_test(x):\n",
    "    n = 1\n",
    "    t = timewindow\n",
    "    d = 40\n",
    "    ret = np.zeros([n, t, 40], dtype=float)   \n",
    "    length=len(x)\n",
    "    if length<t:\n",
    "        ret[:, :length,:] = x\n",
    "    else:\n",
    "        ret[0,:,:] = x[length-t:length,:]         \n",
    "    return ret\n",
    "\n",
    "def get_Normalization_test(X):\n",
    "    X_mean=np.array([84.54736749058092, 97.19735567887244, 36.97384942767003, 123.79455870514964, 82.39844634983528, \n",
    "                     63.82168093649573, 18.720210390214252, 33.01391751459007, -0.6869669057468529, 24.07165352824123,\n",
    "                     0.5629699098557109, 7.378730326999917, 41.08205268057132, 92.6638691105628, 269.71012201748755, \n",
    "                     23.989203500427596, 103.83241100807658, 7.578440945745791, 105.83631589184984, 1.5132853634707293, \n",
    "                     1.7982557651991606, 137.10754340757103, 2.666527938225818, 2.0523013086498567, 3.5532881933995384, \n",
    "                     4.138466653390937, 2.1437651299155416, 8.152533277169335, 30.79291672353087, 10.43255372477224, \n",
    "                     41.26944061491329, 11.458261650095405, 284.57097280966764, 196.1678411769487, 62.02033203772457, \n",
    "                     0.5577890176790509, 0.49908857795492095, 0.5009114220450791, -55.728074496237774, 26.93293913918774]) \n",
    "    \n",
    "    X_std=np.array([17.357143533110786, 2.9197410787621823, 0.769008502635353, 23.226370766088404, 16.35175339480381, \n",
    "                   13.990094006590905, 5.096634488705786, 7.901376008166723, 4.288142385817428, 4.3942700623947095, \n",
    "                   12.460897950458373, 0.07483160020802701, 9.327999041120131, 10.841774762519147, 881.945227133347,\n",
    "                   20.067454723313723, 125.78376567914378, 2.420980582249712, 5.856066574937517, 1.8016280879794837,\n",
    "                   3.555075159474239, 51.31864530748081, 2.5840546941276425, 0.4008345314433298, 1.438200307219221, \n",
    "                   0.6449469067770479, 4.358151926253383, 24.455704359390893, 5.491328159692898, 1.9666320193417495, \n",
    "                   26.26311427074314, 7.737594668485472, 150.0117749185703, 103.8931301983612, 16.41032814994427, \n",
    "                   0.49664920158567677, 0.4999991693091656, 0.4999991693091656, 161.2959477283614, 28.699759200381497])\n",
    "    \n",
    "    X = (X - X_mean)/X_std\n",
    "    return X\n",
    "\n",
    "\n",
    "def get_sepsis_score(data, model,threshold): \n",
    "    if len(data.shape)==1:\n",
    "        data=data.reshape(1,-1)\n",
    "\n",
    "    pd_data=pd.DataFrame(data)\n",
    "    X_last_obsv= np.array(pd_data.fillna(method='ffill').fillna(0))\n",
    "    X_t=data.astype(float)\n",
    "    X_t_mask = 1-np.isnan(X_t).astype('int8')\n",
    "      \n",
    "    X_t[np.isnan(X_t)] = 0\n",
    "    deltaT_t= np.zeros_like(X_t, dtype=int)\n",
    "    deltaT_t[0,:] = 0\n",
    "    for i_t in range(1, len(X_t)):\n",
    "        deltaT_t[i_t,:] = 1 + (1-X_t_mask[i_t,:]) * deltaT_t[i_t-1,:]\n",
    "    \n",
    "    X_t_test=pad_matrix_test(get_Normalization_test(X_t))\n",
    "    X_last_obsv_test=pad_matrix_test(get_Normalization_test(X_last_obsv))\n",
    "\n",
    "    X_t_mask_test=pad_matrix_test(X_t_mask)\n",
    "    deltaT_t_test=pad_matrix_test(deltaT_t)\n",
    "    \n",
    "    test_data=( X_t_test, X_last_obsv_test,X_t_mask_test,deltaT_t_test)\n",
    "    \n",
    "    scores= model.predict(test_data) \n",
    "    \n",
    "    length=len(data)\n",
    "    seq_len=np.where(length>timewindow,timewindow, length)\n",
    "    \n",
    "    scores=scores[:,seq_len-1,:]\n",
    "    \n",
    "    labels=(scores>threshold)     \n",
    "    return (scores, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T10:07:05.729110Z",
     "start_time": "2019-07-29T10:07:05.715354Z"
    }
   },
   "source": [
    "# 6、测试utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T04:14:40.095527Z",
     "start_time": "2019-08-06T04:14:40.005379Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_column(filename, header, delimiter):\n",
    "    column = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for i, l in enumerate(f):\n",
    "            arrs = l.strip().split(delimiter)\n",
    "            if i == 0:\n",
    "                try:\n",
    "                    j = arrs.index(header)\n",
    "                except:\n",
    "                    raise Exception('{} must contain column with header {} containing numerical entries.'.format(filename, header))\n",
    "            else:\n",
    "                if len(arrs[j]):\n",
    "                    column.append(float(arrs[j]))\n",
    "    return np.array(column)\n",
    "\n",
    "def compute_auc(labels, predictions, check_errors=True):\n",
    "    # Check inputs for errors.\n",
    "    if check_errors:\n",
    "        if len(predictions) != len(labels):\n",
    "            raise Exception('Numbers of predictions and labels must be the same.')\n",
    "\n",
    "        for label in labels:\n",
    "            if not label in (0, 1):\n",
    "                raise Exception('Labels must satisfy label == 0 or label == 1.')\n",
    "\n",
    "        for prediction in predictions:\n",
    "            if not 0 <= prediction <= 1:\n",
    "                warnings.warn('Predictions do not satisfy 0 <= prediction <= 1.')\n",
    "\n",
    "    # Find prediction thresholds.\n",
    "    thresholds = np.unique(predictions)[::-1]\n",
    "    if thresholds[0] != 1:\n",
    "        thresholds = np.insert(thresholds, 0, 1)\n",
    "    if thresholds[-1] == 0:\n",
    "        thresholds = thresholds[:-1]\n",
    "\n",
    "    n = len(labels)\n",
    "    m = len(thresholds)\n",
    "\n",
    "    # Populate contingency table across prediction thresholds.\n",
    "    tp = np.zeros(m)\n",
    "    fp = np.zeros(m)\n",
    "    fn = np.zeros(m)\n",
    "    tn = np.zeros(m)\n",
    "\n",
    "    # Find indices that sort the predicted probabilities from largest to\n",
    "    # smallest.\n",
    "    idx = np.argsort(predictions)[::-1]\n",
    "\n",
    "    i = 0\n",
    "    for j in range(m):\n",
    "        # Initialize contingency table for j-th prediction threshold.\n",
    "        if j == 0:\n",
    "            tp[j] = 0\n",
    "            fp[j] = 0\n",
    "            fn[j] = np.sum(labels)\n",
    "            tn[j] = n - fn[j]\n",
    "        else:\n",
    "            tp[j] = tp[j - 1]\n",
    "            fp[j] = fp[j - 1]\n",
    "            fn[j] = fn[j - 1]\n",
    "            tn[j] = tn[j - 1]\n",
    "\n",
    "        # Update contingency table for i-th largest predicted probability.\n",
    "        while i < n and predictions[idx[i]] >= thresholds[j]:\n",
    "            if labels[idx[i]]:\n",
    "                tp[j] += 1\n",
    "                fn[j] -= 1\n",
    "            else:\n",
    "                fp[j] += 1\n",
    "                tn[j] -= 1\n",
    "            i += 1\n",
    "\n",
    "    # Summarize contingency table.\n",
    "    tpr = np.zeros(m)\n",
    "    tnr = np.zeros(m)\n",
    "    ppv = np.zeros(m)\n",
    "    npv = np.zeros(m)\n",
    "\n",
    "    for j in range(m):\n",
    "        if tp[j] + fn[j]:\n",
    "            tpr[j] = tp[j] / (tp[j] + fn[j])\n",
    "        else:\n",
    "            tpr[j] = 1\n",
    "        if fp[j] + tn[j]:\n",
    "            tnr[j] = tn[j] / (fp[j] + tn[j])\n",
    "        else:\n",
    "            tnr[j] = 1\n",
    "        if tp[j] + fp[j]:\n",
    "            ppv[j] = tp[j] / (tp[j] + fp[j])\n",
    "        else:\n",
    "            ppv[j] = 1\n",
    "        if fn[j] + tn[j]:\n",
    "            npv[j] = tn[j] / (fn[j] + tn[j])\n",
    "        else:\n",
    "            npv[j] = 1\n",
    "\n",
    "    auroc = 0\n",
    "    auprc = 0\n",
    "    for j in range(m-1):\n",
    "        auroc += 0.5 * (tpr[j + 1] - tpr[j]) * (tnr[j + 1] + tnr[j])\n",
    "        auprc += (tpr[j + 1] - tpr[j]) * ppv[j + 1]\n",
    "\n",
    "    return auroc, auprc\n",
    "\n",
    "\n",
    "def compute_accuracy_f_measure(labels, predictions, check_errors=True):\n",
    "    # Check inputs for errors.\n",
    "    if check_errors:\n",
    "        if len(predictions) != len(labels):\n",
    "            raise Exception('Numbers of predictions and labels must be the same.')\n",
    "\n",
    "        for label in labels:\n",
    "            if not label in (0, 1):\n",
    "                raise Exception('Labels must satisfy label == 0 or label == 1.')\n",
    "\n",
    "        for prediction in predictions:\n",
    "            if not prediction in (0, 1):\n",
    "                raise Exception('Predictions must satisfy prediction == 0 or prediction == 1.')\n",
    "\n",
    "    # Populate contingency table.\n",
    "    n = len(labels)\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tn = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        if labels[i] and predictions[i]:\n",
    "            tp += 1\n",
    "        elif not labels[i] and predictions[i]:\n",
    "            fp += 1\n",
    "        elif labels[i] and not predictions[i]:\n",
    "            fn += 1\n",
    "        elif not labels[i] and not predictions[i]:\n",
    "            tn += 1\n",
    "\n",
    "    # Summarize contingency table.\n",
    "    if tp + fp + fn + tn:\n",
    "        accuracy = float(tp + tn) / float(tp + fp + fn + tn)\n",
    "    else:\n",
    "        accuracy = 1.0\n",
    "\n",
    "    if 2 * tp + fp + fn:\n",
    "        f_measure = float(2 * tp) / float(2 * tp + fp + fn)\n",
    "    else:\n",
    "        f_measure = 1.0\n",
    "\n",
    "    return accuracy, f_measure\n",
    "\n",
    "def compute_prediction_utility(labels, predictions, dt_early=-12, dt_optimal=-6, dt_late=3.0, max_u_tp=1, min_u_fn=-2, u_fp=-0.05, u_tn=0, check_errors=True):\n",
    "    # Check inputs for errors.\n",
    "    if check_errors:\n",
    "        if len(predictions) != len(labels):\n",
    "            raise Exception('Numbers of predictions and labels must be the same.')\n",
    "\n",
    "        for label in labels:\n",
    "            if not label in (0, 1):\n",
    "                raise Exception('Labels must satisfy label == 0 or label == 1.')\n",
    "\n",
    "        for prediction in predictions:\n",
    "            if not prediction in (0, 1):\n",
    "                raise Exception('Predictions must satisfy prediction == 0 or prediction == 1.')\n",
    "\n",
    "        if dt_early >= dt_optimal:\n",
    "            raise Exception('The earliest beneficial time for predictions must be before the optimal time.')\n",
    "\n",
    "        if dt_optimal >= dt_late:\n",
    "            raise Exception('The optimal time for predictions must be before the latest beneficial time.')\n",
    "\n",
    "    # Does the patient eventually have sepsis?\n",
    "    if np.any(labels):\n",
    "        is_septic = True\n",
    "        t_sepsis = np.argmax(labels) - dt_optimal     ##########################之前的评分没有dt_optimal这一项\n",
    "    else:\n",
    "        is_septic = False\n",
    "        t_sepsis = float('inf')\n",
    "\n",
    "    n = len(labels)\n",
    "\n",
    "    # Define slopes and intercept points for utility functions of the form\n",
    "    # u = m * t + b.\n",
    "    m_1 = float(max_u_tp) / float(dt_optimal - dt_early)\n",
    "    b_1 = -m_1 * dt_early\n",
    "    m_2 = float(-max_u_tp) / float(dt_late - dt_optimal)\n",
    "    b_2 = -m_2 * dt_late\n",
    "    m_3 = float(min_u_fn) / float(dt_late - dt_optimal)\n",
    "    b_3 = -m_3 * dt_optimal\n",
    "\n",
    "    # Compare predicted and true conditions.\n",
    "    u = np.zeros(n)\n",
    "    for t in range(n):\n",
    "        if t <= t_sepsis + dt_late:\n",
    "            # TP\n",
    "            if is_septic and predictions[t]:\n",
    "                if t <= t_sepsis + dt_optimal:\n",
    "                    u[t] = max(m_1 * (t - t_sepsis) + b_1, u_fp)\n",
    "                elif t <= t_sepsis + dt_late:\n",
    "                    u[t] = m_2 * (t - t_sepsis) + b_2\n",
    "            # FP\n",
    "            elif not is_septic and predictions[t]:\n",
    "                u[t] = u_fp\n",
    "            # FN\n",
    "            elif is_septic and not predictions[t]:\n",
    "                if t <= t_sepsis + dt_optimal:\n",
    "                    u[t] = 0\n",
    "                elif t <= t_sepsis + dt_late:\n",
    "                    u[t] = m_3 * (t - t_sepsis) + b_3\n",
    "            # TN\n",
    "            elif not is_septic and not predictions[t]:\n",
    "                u[t] = u_tn\n",
    "\n",
    "    # Find total utility for patient.\n",
    "    return np.sum(u)\n",
    "\n",
    "def evaluate_scores(label_directory, prediction_directory):\n",
    "    # Set parameters.\n",
    "    label_header       = 'SepsisLabel'\n",
    "    prediction_header  = 'PredictedLabel'\n",
    "    probability_header = 'PredictedProbability'\n",
    "\n",
    "    dt_early   = -12\n",
    "    dt_optimal = -6\n",
    "    dt_late    = 3\n",
    "\n",
    "    max_u_tp = 1\n",
    "    min_u_fn = -2\n",
    "    u_fp     = -0.05\n",
    "    u_tn     = 0\n",
    "\n",
    "    # Find label and prediction files.\n",
    "    label_files = []\n",
    "    for f in os.listdir(label_directory):\n",
    "        g = os.path.join(label_directory, f)\n",
    "        if os.path.isfile(g) and not f.lower().startswith('.') and f.lower().endswith('psv'):\n",
    "            label_files.append(g)\n",
    "    label_files = sorted(label_files)\n",
    "\n",
    "    prediction_files = []\n",
    "    for f in os.listdir(prediction_directory):\n",
    "        g = os.path.join(prediction_directory, f)\n",
    "        if os.path.isfile(g) and not f.lower().startswith('.') and f.lower().endswith('psv'):\n",
    "            prediction_files.append(g)\n",
    "    prediction_files = sorted(prediction_files)\n",
    "\n",
    "    if len(label_files) != len(prediction_files):\n",
    "        raise Exception('Numbers of label and prediction files must be the same.')\n",
    "\n",
    "    # Load labels and predictions.\n",
    "    num_files            = len(label_files)\n",
    "    cohort_labels        = []\n",
    "    cohort_predictions   = []\n",
    "    cohort_probabilities = []\n",
    "\n",
    "    for k in range(num_files):\n",
    "        labels        = load_column(label_files[k], label_header, '|')\n",
    "        predictions   = load_column(prediction_files[k], prediction_header, '|')\n",
    "        probabilities = load_column(prediction_files[k], probability_header, '|')\n",
    "\n",
    "        # Check labels and predictions for errors.\n",
    "        if not (len(labels) == len(predictions) and len(predictions) == len(probabilities)):\n",
    "            raise Exception('Numbers of labels and predictions for a file must be the same.')\n",
    "\n",
    "        num_rows = len(labels)\n",
    "\n",
    "        for i in range(num_rows):\n",
    "            if labels[i] not in (0, 1):\n",
    "                raise Exception('Labels must satisfy label == 0 or label == 1.')\n",
    "\n",
    "            if predictions[i] not in (0, 1):\n",
    "                raise Exception('Predictions must satisfy prediction == 0 or prediction == 1.')\n",
    "\n",
    "            if not 0 <= probabilities[i] <= 1:\n",
    "                warnings.warn('Probabilities do not satisfy 0 <= probability <= 1.')\n",
    "\n",
    "        if 0 < np.sum(predictions) < num_rows:\n",
    "            min_probability_positive = np.min(probabilities[predictions == 1])\n",
    "            max_probability_negative = np.max(probabilities[predictions == 0])\n",
    "\n",
    "            if min_probability_positive <= max_probability_negative:\n",
    "                warnings.warn('Predictions are inconsistent with probabilities, i.e., a positive prediction has a lower (or equal) probability than a negative prediction.')\n",
    "\n",
    "        # Record labels and predictions.\n",
    "        cohort_labels.append(labels)\n",
    "        cohort_predictions.append(predictions)\n",
    "        cohort_probabilities.append(probabilities)\n",
    "\n",
    "    # Compute AUC, accuracy, and F-measure.\n",
    "    labels        = np.concatenate(cohort_labels)\n",
    "    predictions   = np.concatenate(cohort_predictions)\n",
    "    probabilities = np.concatenate(cohort_probabilities)\n",
    "\n",
    "    auroc, auprc        = compute_auc(labels, probabilities)\n",
    "    accuracy, f_measure = compute_accuracy_f_measure(labels, predictions)\n",
    "\n",
    "    # Compute utility.\n",
    "    observed_utilities = np.zeros(num_files)\n",
    "    best_utilities     = np.zeros(num_files)\n",
    "    worst_utilities    = np.zeros(num_files)\n",
    "    inaction_utilities = np.zeros(num_files)\n",
    "\n",
    "    for k in range(num_files):\n",
    "        labels = cohort_labels[k]\n",
    "        num_rows          = len(labels)\n",
    "        observed_predictions = cohort_predictions[k]\n",
    "        best_predictions     = np.zeros(num_rows)\n",
    "        worst_predictions    = np.zeros(num_rows)\n",
    "        inaction_predictions = np.zeros(num_rows)\n",
    "\n",
    "        if np.any(labels):\n",
    "            t_sepsis = np.argmax(labels) - dt_optimal                            ###############之前没有dt_optimal这一项\n",
    "            best_predictions[max(0, t_sepsis + dt_early) : min(t_sepsis + dt_late + 1, num_rows)] = 1\n",
    "        worst_predictions = 1 - best_predictions\n",
    "\n",
    "        observed_utilities[k] = compute_prediction_utility(labels, observed_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "        best_utilities[k]     = compute_prediction_utility(labels, best_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "        worst_utilities[k]    = compute_prediction_utility(labels, worst_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "        inaction_utilities[k] = compute_prediction_utility(labels, inaction_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "\n",
    "    unnormalized_observed_utility = np.sum(observed_utilities)\n",
    "    unnormalized_best_utility     = np.sum(best_utilities)\n",
    "    unnormalized_worst_utility    = np.sum(worst_utilities)\n",
    "    unnormalized_inaction_utility = np.sum(inaction_utilities)\n",
    "    normalized_observed_utility = (unnormalized_observed_utility - unnormalized_inaction_utility) / (unnormalized_best_utility - unnormalized_inaction_utility)\n",
    "    return auroc, auprc, accuracy, f_measure, normalized_observed_utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T10:52:28.676799Z",
     "start_time": "2019-08-06T04:20:01.455534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing the 0 person\n",
      "processing the 1000 person\n",
      "processing the 2000 person\n",
      "processing the 3000 person\n",
      "processing the 4000 person\n",
      "processing the 5000 person\n",
      "processing the 6000 person\n",
      "processing the 7000 person\n",
      "processing the 8000 person\n",
      "cutoff is 8066\n",
      "AUROC|AUPRC|Accuracy|F-measure|Utility\n",
      "0.7963271023904925|0.09172217207448993|0.760414365790493|0.09346156641391506|0.316248188151681\n",
      "processing the 0 person\n",
      "processing the 1000 person\n",
      "processing the 2000 person\n",
      "processing the 3000 person\n",
      "processing the 4000 person\n",
      "processing the 5000 person\n",
      "processing the 6000 person\n",
      "processing the 7000 person\n",
      "processing the 8000 person\n",
      "cutoff is 8066\n",
      "AUROC|AUPRC|Accuracy|F-measure|Utility\n",
      "0.7963271023904925|0.09172217207448993|0.7768462230417343|0.0971623214308842|0.3258901435451443\n",
      "processing the 0 person\n",
      "processing the 1000 person\n",
      "processing the 2000 person\n",
      "processing the 3000 person\n",
      "processing the 4000 person\n",
      "processing the 5000 person\n",
      "processing the 6000 person\n",
      "processing the 7000 person\n",
      "processing the 8000 person\n",
      "cutoff is 8066\n",
      "AUROC|AUPRC|Accuracy|F-measure|Utility\n",
      "0.7963271023904925|0.09172217207448993|0.7887515565927505|0.10009545888449475|0.3306816056482911\n",
      "processing the 0 person\n",
      "processing the 1000 person\n",
      "processing the 2000 person\n",
      "processing the 3000 person\n",
      "processing the 4000 person\n",
      "processing the 5000 person\n",
      "processing the 6000 person\n",
      "processing the 7000 person\n",
      "processing the 8000 person\n",
      "cutoff is 8066\n",
      "AUROC|AUPRC|Accuracy|F-measure|Utility\n",
      "0.7963271023904925|0.09172217207448993|0.7993123781536009|0.10301755590848607|0.3349558142797026\n",
      "processing the 0 person\n",
      "processing the 1000 person\n",
      "processing the 2000 person\n",
      "processing the 3000 person\n",
      "processing the 4000 person\n",
      "processing the 5000 person\n",
      "processing the 6000 person\n",
      "processing the 7000 person\n",
      "processing the 8000 person\n",
      "cutoff is 8066\n",
      "AUROC|AUPRC|Accuracy|F-measure|Utility\n",
      "0.7963271023904925|0.09172217207448993|0.808803992560367|0.10600526883008023|0.33978818908682845\n",
      "processing the 0 person\n",
      "processing the 1000 person\n",
      "processing the 2000 person\n",
      "processing the 3000 person\n",
      "processing the 4000 person\n",
      "processing the 5000 person\n",
      "processing the 6000 person\n",
      "processing the 7000 person\n",
      "processing the 8000 person\n",
      "cutoff is 8066\n",
      "AUROC|AUPRC|Accuracy|F-measure|Utility\n",
      "0.7963271023904925|0.09172217207448993|0.8174888997730335|0.10821041435297431|0.3394234815542152\n"
     ]
    }
   ],
   "source": [
    "filetype ='.psv'\n",
    "test_path='/data/jiawenxiao/physionet0727/test_data/'\n",
    "output_path='/data/jiawenxiao/physionet0727/test_output/'\n",
    "test_name=get_filename(test_path,filetype)\n",
    "\n",
    "cutoff=0.020\n",
    "\n",
    "for k in cutoff:\n",
    "    for record_name in  test_name:\n",
    "        input_file = test_path+record_name    \n",
    "        data= pd.read_csv(input_file,sep='|')  \n",
    "        data=np.array(data.iloc[:,1:-3])\n",
    "\n",
    "        num_rows = len(data)\n",
    "        scores = np.zeros(num_rows)\n",
    "        labels = np.zeros(num_rows)\n",
    "        for t in range(num_rows):  \n",
    "            raw_data = data[:t+1]    \n",
    "            current_score, current_label = get_sepsis_score(raw_data,model,k)\n",
    "            scores[t] = current_score\n",
    "            labels[t] = current_label \n",
    "\n",
    "        record_name=record_name[:-4]\n",
    "        output_file = output_path+record_name + '.psv'\n",
    "        save_challenge_predictions(output_file, scores, labels)    \n",
    "\n",
    "    auroc, auprc, accuracy, f_measure, utility = evaluate_scores(test_path, output_path)\n",
    "    print('cutoff is '+str(k))\n",
    "    output_string = 'AUROC|AUPRC|Accuracy|F-measure|Utility\\n{}|{}|{}|{}|{}'.format(auroc, auprc, accuracy, f_measure, utility)\n",
    "    print(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
